{"cells":[{"metadata":{"_uuid":"97364905a6477eb4c8f0c5e9e0721a7855b1c1e4","_cell_guid":"1f327477-f303-45b5-b98f-318ae2eea23e"},"cell_type":"markdown","source":"The purpose of this notebook is to try out a few algorithms for classification problems. I will start with some pre processing (scanling the data, basically) and then cover the following algorithms: Suport vector Classification (SVM), K-nn, XGBoost, Decision Tree and Random Forest. For each one, I will compute the model accuracy both for the test and train dataset. I used this good notebook (https://www.kaggle.com/mgabrielkerr/visualizing-knn-svm-and-xgboost-on-iris-dataset) as my starting point. "},{"metadata":{"_uuid":"cbe7b0a58e5aa126b662f0744884174bc33349c7","_cell_guid":"38e2fbb1-b421-4954-b858-6c4519c40d99","collapsed":true,"trusted":false},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n%pylab inline\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"14236a81f5c3948b944b9f0cc80b67895b9f22d7","_cell_guid":"cb64d365-0bf0-4f4b-b14e-3eec47ccb2ee","collapsed":true,"trusted":false},"cell_type":"code","source":"#Reading data from CSV file\ndf = pd.read_csv(\"../input/Iris.csv\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"63c7aed58c5b4d69a252f6837b5f27c1ae6a86a7","_cell_guid":"f09e1eb1-89d4-44c9-9b99-a72fae792ddf","collapsed":true,"trusted":false},"cell_type":"code","source":"#Defining data and label\nX = df.iloc[:, 1:5]\ny = df.iloc[:, 5]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e602c21b372b8ebe8ba0cb636777e130c062dd41","_cell_guid":"a4d2019e-4973-4dc2-a228-08209005e712","collapsed":true,"trusted":false},"cell_type":"code","source":"#Split data into training and test datasets (training will be based on 70% of data)\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0) \n#test_size: if integer, number of examples into test dataset; if between 0.0 and 1.0, means proportion\nprint('There are {} samples in the training set and {} samples in the test set'.format(X_train.shape[0], X_test.shape[0]))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0f3eecc383edbc105ffabfb9ec208c53817c6869","_cell_guid":"5df96dde-5545-468b-b039-a3941cff6ac3","collapsed":true,"trusted":false},"cell_type":"code","source":"#Scaling data\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import cross_val_score\n\nsc = StandardScaler()\nsc.fit(X_train)\nX_train_std = sc.transform(X_train)\nX_test_std = sc.transform(X_test)\n\n#X_train_std and X_test_std are the scaled datasets to be used in algorithms","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3bc58831329c6ccbde862b99abefb9fab7184ee9","_cell_guid":"f7fd7c56-84eb-4a72-b09c-ff03ac181b50","collapsed":true,"trusted":false},"cell_type":"code","source":"#Applying SVC (Support Vector Classification)\nfrom sklearn.svm import SVC\n\nsvm = SVC(kernel='rbf', random_state=0, gamma=.10, C=1.0)\nsvm.fit(X_train_std, y_train)\nprint('The accuracy of the SVM classifier on training data is {:.2f}'.format(svm.score(X_train_std, y_train)))\nprint('The accuracy of the SVM classifier on test data is {:.2f}'.format(svm.score(X_test_std, y_test)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"15a70b05253ba91d31cebf3c5f350fc4a71a0b21","_cell_guid":"3ec26cfc-ddc6-41b5-821d-8c6bfbf41816","collapsed":true,"trusted":false},"cell_type":"code","source":"#Applying Knn\nfrom sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier(n_neighbors = 7, p = 2, metric='minkowski')\nknn.fit(X_train_std, y_train)\n\nprint('The accuracy of the Knn classifier on training data is {:.2f}'.format(knn.score(X_train_std, y_train)))\nprint('The accuracy of the Knn classifier on test data is {:.2f}'.format(knn.score(X_test_std, y_test)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6d5030e85484e8dac943f74239969134a14a89f5","_cell_guid":"f2f299f9-da37-4770-9f8b-9661fcbe7e42","collapsed":true,"trusted":false},"cell_type":"code","source":"#Applying XGBoost\nimport xgboost as xgb\n\nxgb_clf = xgb.XGBClassifier()\nxgb_clf = xgb_clf.fit(X_train_std, y_train)\n\nprint('The accuracy of the XGBoost classifier on training data is {:.2f}'.format(xgb_clf.score(X_train_std, y_train)))\nprint('The accuracy of the XGBoost classifier on test data is {:.2f}'.format(xgb_clf.score(X_test_std, y_test)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"69ec407bbca8b49f02e6826b71469e4e8c4f21ad","_cell_guid":"4e3354ce-6c65-42af-8e53-0d59289011e5","collapsed":true,"trusted":false},"cell_type":"code","source":"#Applying Decision Tree\nfrom sklearn import tree\n\n#Create tree object\ndecision_tree = tree.DecisionTreeClassifier(criterion='gini')\n\n#Train DT based on scaled training set\ndecision_tree.fit(X_train_std, y_train)\n\n#Print performance\nprint('The accuracy of the Decision Tree classifier on training data is {:.2f}'.format(decision_tree.score(X_train_std, y_train)))\nprint('The accuracy of the Decision Tree classifier on test data is {:.2f}'.format(decision_tree.score(X_test_std, y_test)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bc9809d06edc2a4374a08c601836befc3652e7b9","_cell_guid":"739f6b0a-fd06-466c-8a24-64a82a64ced0","collapsed":true,"trusted":false},"cell_type":"code","source":"#Applying RandomForest\nfrom sklearn.ensemble import RandomForestClassifier\n\n#Create Random Forest object\nrandom_forest = RandomForestClassifier()\n\n#Train model\nrandom_forest.fit(X_train_std, y_train)\n\n#Print performance\nprint('The accuracy of the Random Forest classifier on training data is {:.2f}'.format(random_forest.score(X_train_std, y_train)))\nprint('The accuracy of the Random Forest classifier on test data is {:.2f}'.format(random_forest.score(X_test_std, y_test)))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fa134e20a45b811c28e5257fe8c03cf07459a8bf","_cell_guid":"f55f9eae-0e61-42d3-bc7e-7571703200df"},"cell_type":"markdown","source":"**Conclusion**\nThis notebook explored 5 basic machine learning algorithms. In the majority of them, the accuracy was higher in the training dataset, as expected. In the ones it was lower, the difference in not significant. The accuracy for those cases should be evaluated with a cross validation and not only with a single fold. Using cross validation will probably result in higher accuracy for all algorithms. This dataset is very simple, so maybe we cannot note significant improvement in the algorithms.\n\n**Future Developments**\nAs future work, I intent to implement all algoritms with cross validation in order to obtain a more consistent accuracy value. Also, I would like to explore how is the behavior of the performance changes if I change some parameters in the models (e.g. how the value of k changes the performance of K-nn classifier? Which k is the optimal? and so on for other classifiers). Another next step is to explore the decision boundaries for each algorithm. Such work has been done by \"Anisotropic\" in his notebook (https://www.kaggle.com/arthurtok/decision-boundaries-visualised-via-python-plotly). "}],"metadata":{"language_info":{"nbconvert_exporter":"python","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"version":"3.6.3","name":"python","mimetype":"text/x-python","pygments_lexer":"ipython3"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}